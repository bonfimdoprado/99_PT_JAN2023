{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sns.set_theme(context = 'notebook', style = 'darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de Classificação II\n",
    "\n",
    "Vamos avançar nossos estudos sobre métodos de classificação analisando dois novos algoritmos da categoria de *Modelos de Ensemble* (*bagging* e *boosting*). Além disso teremos o primeiro contato com os conceitos de *underfitting* e *overfitting*, e como esses conceitos se relacionam com a **complexidade** dos modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_hotel_train = pd.read_csv(\"data/tb_hotel_train_clean.csv\")\n",
    "tb_hotel_test = pd.read_csv(\"data/tb_hotel_test_clean.csv\")\n",
    "tb_hotel_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = [\"lead_time\", \"adr\"]\n",
    "X_train = tb_hotel_train[x_var]\n",
    "X_test = tb_hotel_test[x_var]\n",
    "y_train = tb_hotel_train[\"is_cancelled\"]\n",
    "y_test = tb_hotel_test[\"is_cancelled\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para facilitar a visualização dos nossos modelos continuaremos nos restringindo à duas variáveis: `lead_time` e `adr`. Vamos criar dois DataFrames para visualizar separadamente os efeitos de cada variável sobre **a função de probabilidade** estimada por cada algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time_simul = list(np.linspace(-1, 3, 100)) * 3\n",
    "adr_simul = [-1] * 100 + [0] * 100 + [1] * 100\n",
    "tb_simul_lt = pd.DataFrame({'lead_time' : lead_time_simul, 'adr' : adr_simul})\n",
    "tb_simul_lt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_simul = list(np.linspace(-2, 2, 100)) * 3\n",
    "lead_time_simul = [-1] * 100 + [0] * 100 + [1] * 100\n",
    "tb_simul_adr = pd.DataFrame({'lead_time' : lead_time_simul, 'adr' : adr_simul})\n",
    "tb_simul_adr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar um DataFrame com os dados originais de teste para guardarmos as diferentes previsões de nossos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_fits = X_test.copy()\n",
    "tb_fits[\"is_cancelled\"] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexidade e Overfitting\n",
    "\n",
    "Para avaliarmos melhor a relação entre complexidade e overfitting, vamos carregar mais variáveis de nosso dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_of = pd.read_csv('data/tb_hotel_train_overfit.csv')\n",
    "test_of = pd.read_csv('data/tb_hotel_test_overfit.csv')\n",
    "\n",
    "X_train_of = train_of.drop('is_cancelled', axis = 1)\n",
    "X_test_of = test_of.drop('is_cancelled', axis = 1)\n",
    "y_train_of = train_of['is_cancelled']\n",
    "y_test_of = test_of['is_cancelled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos no começo do Módulo III, podemos dividir a utilização dos algoritmos de ML em duas etapas:\n",
    "\n",
    "* **Aprendizagem**, onde o algoritmo *aprende* as relações entre nossas *features* e a *variável resposta* utilizando dados históricos (representados pelo conjunto de treinamento);\n",
    "* **Predição**, onde utilizamos os padrões *aprendidos* pelo algoritmo para realizar projeções sobre novos dados a partir de nossos *features* (represetado pelo conjunto de teste).\n",
    "\n",
    "A **fase de aprendizagem** consiste na otimização do erro de projeção sobre o conjunto de treinamento - o modelo ajusta gradualmente seus coeficientes buscando melhorar a cada etapa seu erro de projeção sobre os dados históricos. Conforme aumentamos a **complexidade** do modelo essa otimização torna-se cada vez mais eficiente. \n",
    "\n",
    "Isso não significa, necessariamente, que o erro de previsão do modelo melhorará! Conforme a **complexidade** aumenta, o modelo perde a **capacidade de generalização**: ao invés de *encontrar padrões* nos dados históricos ele aprende regras que se aplicam somente às observações do conjunto de treinamento.\n",
    "\n",
    "Vamos utilizar uma árvore de decisão para visualizar este processo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As árvores de decisão tem um parâmetro de complexidade muito simples, a **profundida máxima**. Vamos utilizar um `loop for` para construir árvores de decisão de diferentes profundidades e avaliar seu erro sobre o conjunto de treinamento e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [int(x) for x in np.linspace(2, 40, 20)]\n",
    "\n",
    "d_list = []\n",
    "f1_train_list = []\n",
    "f1_test_list = []\n",
    "\n",
    "for d in max_depth:\n",
    "    rf_fit = DecisionTreeClassifier(max_depth= d)\n",
    "    rf_fit.fit(X_train_of, y_train_of)\n",
    "    y_pred_test = rf_fit.predict(X_test_of)\n",
    "    y_pred_train = rf_fit.predict(X_train_of)\n",
    "\n",
    "    f1_test = np.round(f1_score(y_test_of, y_pred_test), 4)\n",
    "    f1_train = np.round(f1_score(y_train_of, y_pred_train), 4)\n",
    "\n",
    "\n",
    "    d_list.append(d)\n",
    "    f1_train_list.append(f1_train)\n",
    "    f1_test_list.append(f1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_rf_fit = pd.DataFrame(\n",
    "    {\n",
    "        'depth' : d_list,\n",
    "        'f1_train' : f1_train_list,\n",
    "        'f1_test' : f1_test_list\n",
    "    }\n",
    ")\n",
    "tb_rf_fit['diff_error'] = tb_rf_fit['f1_train'] - tb_rf_fit['f1_test']\n",
    "tb_rf_fit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos comparar a evolução do erro sobre os dois conjuntos, teste e treinamento, para visualizar o impacto da complexidade (representada pela profundidade) sobre underfitting/overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data = tb_rf_fit, x = 'depth', y = 'f1_train')\n",
    "sns.lineplot(data = tb_rf_fit, x = 'depth', y = 'f1_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relação entre Tradeoff Viés-Variância e Overfitting\n",
    "\n",
    "O tradeoff viés-variância está intimamente relacionado ao problema de overfitting em aprendizado de máquina.\n",
    "\n",
    "- **Viés** refere-se ao erro introduzido ao aproximar um problema do mundo real com um modelo simplificado. Isso significa que um modelo com alto viés é provavelmente fazer suposições incorretas sobre as relações subjacentes nos dados, levando a underfitting e baixo desempenho preditivo.\n",
    "\n",
    "- **Variância** refere-se à quantidade pela qual as previsões de um modelo mudariam se fosse treinado em um conjunto diferente de dados. Um modelo com alta variância é provavelmente capturar ruídos aleatórios nos dados de treinamento, levando a overfitting e má generalização para novos dados.\n",
    "\n",
    "O tradeoff entre viés e variância surge porque modelos com mais complexidade podem ajustar os dados de treinamento mais precisamente (reduzindo o viés), mas são mais propensos a overfitting (aumentando a variância). Da mesma forma, modelos com menos complexidade são menos propensos a overfitting, mas podem não capturar todos os padrões relevantes nos dados (aumentando o viés).\n",
    "\n",
    "Overfitting ocorre quando um modelo é muito complexo em relação à quantidade de dados de treinamento disponíveis, resultando em um modelo que ajusta os dados de treinamento muito bem, mas generaliza mal para novos dados não vistos. \n",
    "\n",
    "Técnicas de ensembling como bagging e boosting também podem ajudar a reduzir o overfitting, combinando vários modelos que são menos propensos a overfitting do que um único modelo mais complexo. \n",
    "\n",
    "- **Bagging** é uma técnica de ensemble que combina vários modelos independentes, treinados em diferentes subconjuntos aleatórios do conjunto de dados original. Isso reduz a variância do modelo final, porque cada modelo é treinado em um conjunto de dados diferente, tornando o modelo final menos suscetível a ruído aleatório no conjunto de dados.\n",
    "\n",
    "- **Boosting** é uma técnica de ensemble que combina vários modelos fracos em um modelo forte. Ao contrário do bagging, que treina modelos independentes, o boosting treina cada modelo sequencialmente, dando mais peso aos exemplos de treinamento que foram classificados incorretamente pelo modelo anterior. Isso ajuda a reduzir o viés do modelo final, porque cada modelo subsequente é treinado para corrigir os erros do modelo anterior.\n",
    "\n",
    "## Técnicas de Ensemble\n",
    "\n",
    "Ensemble é uma técnica de aprendizado de máquina que combina vários modelos diferentes para melhorar a precisão e a robustez das previsões.\n",
    "\n",
    "### Bagging\n",
    "\n",
    "Bagging (Bootstrap Aggregating) foi proposto por Leo Breiman em 1996 como uma técnica para reduzir a variância dos modelos e evitar o overfitting. \n",
    "\n",
    "A ideia por trás do bagging é:\n",
    "- criar várias instâncias do modelo base (por exemplo, árvore de decisão) treinando cada uma em uma amostra diferente do conjunto de dados de treinamento.\n",
    "- selecionar essas amostras com substituição (bootstrap), o que significa que cada amostra é uma seleção aleatória do conjunto de dados de treinamento original.\n",
    "- combinar as previsões de cada modelo por meio de votação para gerar uma única previsão final.\n",
    "\n",
    "O bagging reduz a variância porque:\n",
    "- as instâncias do modelo base treinadas em diferentes amostras de dados tendem a produzir previsões ligeiramente diferentes.\n",
    "- a votação ajuda a combinar essas previsões em uma única previsão mais robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit = RandomForestClassifier(n_estimators = 1500, max_depth= 3)\n",
    "rf_fit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_fits[\"pred_prob_rf\"] = rf_fit.predict_proba(X_test)[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "sns.scatterplot(\n",
    "    data=tb_fits, \n",
    "    x=\"lead_time\", y=\"pred_prob_rf\", hue = \"adr\", \n",
    "    ax =ax[0], palette='Spectral')\n",
    "sns.scatterplot(\n",
    "    data=tb_fits, \n",
    "    x=\"adr\", y=\"pred_prob_rf\", hue = \"lead_time\", \n",
    "    ax = ax[1], palette = 'Spectral');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot_decision_regions(\n",
    "    np.array(tb_fits[[\"lead_time\", \"adr\"]),\n",
    "    np.array(tb_fits[\"is_cancelled\"]),\n",
    "    rf_fit,\n",
    "    scatter_kwargs={\"alpha\": 0.001},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_simul_lt['pred_prob_rf'] = rf_fit.predict_proba(tb_simul_lt[x_var])[:,1]\n",
    "tb_simul_adr['pred_prob_rf'] = rf_fit.predict_proba(tb_simul_adr[x_var])[:,1]\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "sns.lineplot(\n",
    "    data=tb_simul_lt, \n",
    "    x=\"lead_time\", y=\"pred_prob_rf\", hue = \"adr\", \n",
    "    ax =ax[0], palette='Spectral')\n",
    "sns.lineplot(\n",
    "    data=tb_simul_adr, \n",
    "    x=\"adr\", y=\"pred_prob_rf\", hue = \"lead_time\", \n",
    "    ax = ax[1], palette = 'Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Boosting\n",
    "\n",
    "Boosting foi proposto por Robert Schapire em 1990 e posteriormente desenvolvido por Yoav Freund e outros pesquisadores. \n",
    "\n",
    "O objetivo do boosting é melhorar a precisão do modelo base, enfatizando os exemplos de treinamento que foram mal previstos. \n",
    "\n",
    "A ideia é:\n",
    "- treinar vários modelos fracos (por exemplo, uma árvore de decisão rasa) em diferentes subconjuntos dos dados de treinamento.\n",
    "- atribuir pesos aos exemplos de treinamento com base em quão bem o modelo previu esses exemplos.\n",
    "- os exemplos mal previstos recebem um peso maior, de modo que o modelo seguinte seja mais focado nesses exemplos e possa corrigir os erros anteriores.\n",
    "- a previsão final é feita por meio da combinação das previsões de todos os modelos com base em seus pesos.\n",
    "\n",
    "O boosting reduz o viés do modelo base, tornando-o mais preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fit = CatBoostClassifier(\n",
    "    iterations = 500, depth=4\n",
    "    )\n",
    "cat_fit.fit(X_train, y_train, eval_set = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_fits[\"pred_prob_cat\"] = cat_fit.predict_proba(X_test)[:, 1]\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "sns.scatterplot(\n",
    "    data=tb_fits, \n",
    "    x=\"lead_time\", y=\"pred_prob_cat\", hue = \"adr\", \n",
    "    ax =ax[0], palette='Spectral')\n",
    "sns.scatterplot(\n",
    "    data=tb_fits, \n",
    "    x=\"adr\", y=\"pred_prob_cat\", hue = \"lead_time\", \n",
    "    ax = ax[1], palette = 'Spectral');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plot_decision_regions(\n",
    "    np.array(tb_fits[[\"lead_time\", \"adr\"]]),\n",
    "    np.array(tb_fits[\"is_cancelled\"]),\n",
    "    cat_fit,\n",
    "    scatter_kwargs={\"alpha\": 0.001},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_simul_lt['pred_prob_cat'] = cat_fit.predict_proba(tb_simul_lt[x_var])[:,1]\n",
    "tb_simul_adr['pred_prob_cat'] = cat_fit.predict_proba(tb_simul_adr[x_var])[:,1]\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "sns.lineplot(\n",
    "    data=tb_simul_lt, \n",
    "    x=\"lead_time\", y=\"pred_prob_cat\", hue = \"adr\", \n",
    "    ax =ax[0], palette='Spectral')\n",
    "sns.lineplot(\n",
    "    data=tb_simul_adr, \n",
    "    x=\"adr\", y=\"pred_prob_cat\", hue = \"lead_time\", \n",
    "    ax = ax[1], palette = 'Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha entre Bagging e Boosting\n",
    "\n",
    "Em geral, o bagging é preferido quando o modelo base é propenso a overfitting, enquanto o boosting é preferido quando o modelo base é muito simples e tem alto viés.\n",
    "\n",
    "Ambas as técnicas são muito usadas em problemas de aprendizado de máquina e continuam sendo áreas ativas de pesquisa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "014f4a4a5af8f0104b12c029e500f4146d6d785e8cf714d2a35b7a9514230cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
