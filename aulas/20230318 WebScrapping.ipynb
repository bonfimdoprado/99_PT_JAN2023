{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:57:20.851493Z",
     "start_time": "2022-01-22T13:57:20.057718Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "\n",
    "Muitas vezes os dados que queremos não estão disponibilizados através de APIs, apenas sites. Neste momento precisamos recorrer às ferramentas de **Web Scrapping**!\n",
    "\n",
    "*Web scrapping* é a extração de informação estruturada a partir de paginas na internet: por exemplo, podemos extrair todos os artigos de um jornal que mencionem um certo produto, ou então as informações de uma série de tabelas da Wikipedia.\n",
    "\n",
    "Hoje vamos aprender como utilizar as bibliotecas BeautifulSoup e Selenium para extrair informações a partir de links específicos, realizar buscas e navegar páginas.\n",
    "\n",
    "# Conhecendo o BeautifulSoup\n",
    "\n",
    "Vamos começar extraindo informações básicas a partir de uma notícia do portal UOL. O primeiro passo é utilizar a biblioteca `requests` para *baixar* o html da página:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T13:58:46.297320Z",
     "start_time": "2022-01-22T13:58:46.291357Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.uol.com.br/esporte/futebol/ultimas-noticias/2022/01/22/em-1995-decisao-na-base-entre-palmeiras-x-sp-terminou-em-morte-no-pacaembu.htm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html_str = response.text\n",
    "type(html_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_str[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_bytes = response.content\n",
    "type(html_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(html_bytes[0:1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfome em sopa\n",
    "\n",
    "A página extraída acima é apenas um string: o código HTML. Para utiliza-la dentro do Python, precisamos de um **parser**: um conjunto de funções que nos permite **interpretar** este código HTML e extrair informações relevantes. A biblioteca *BeautifulSoup* implementa um **parser** de HTML dentro do Python, dando acesso à arvore de tags (`<head>`, `<link ...> `, etc).\n",
    "\n",
    "Para utilizar este **parser** precisamos entender um pouco da estrutura de um arquivo HTML. Mas antes vamos utilizar o BeautifulSoup para deixar o *print* de nosso HTML mais organizado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:02:37.459772Z",
     "start_time": "2022-01-22T14:02:37.389848Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:03:30.537353Z",
     "start_time": "2022-01-22T14:03:30.526061Z"
    }
   },
   "outputs": [],
   "source": [
    "type(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:03:47.506802Z",
     "start_time": "2022-01-22T14:03:47.441977Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrando Tags\n",
    "\n",
    "Um arquivo HTML é estruturado em **tags**: marcações com o formato `<nome_do_tag>` e `</nome_do_tag>`. A primeira denota o **inicio do conteúdo** do tag, a segunda o **fim do conteúdo**. Vamos entender como um **tag simples** funciona analisando o conteúdo do `<title>`.\n",
    "\n",
    "Para encontrar todas as ocorrências de um **tag** pelo seu **nome** utilizamos o método `.find_all()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:06:57.412662Z",
     "start_time": "2022-01-22T14:06:57.395729Z"
    }
   },
   "outputs": [],
   "source": [
    "soup.find_all(\"title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método nos retornou todas as ocorrências de `<title>` no HTML (uma só no caso) em uma lista. Vamos ver o que essa lista contém:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find_all(\"title\")[0]\n",
    "print(type(title))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objeto `Tag` da BeautifulSoup contém todas as informações de um tag: atributos, links, conteúdo... Por enquanto vamos olhar o conteúdo desse tag (o que está entre `<title>` e `< \\title>`) utilizando o atributo `.text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(title.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tags** com mais que uma ocorrência \n",
    "\n",
    "A maior parte dos **tags** ocorrem múltiplas vezes em um documento. Vamos buscar um **tag** com essa característica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:13:15.147858Z",
     "start_time": "2022-01-22T14:13:15.136859Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = soup.find_all(\"h3\")\n",
    "print(headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(headers[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Utilize uma list comprehension para criar uma lista com o texto de todos os headers h3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando múltiplos **tags**\n",
    "\n",
    "Além de buscar **tags** um a um, podemos utilizar o método `.find_all()` para encontrar todas as ocorrências de uma lista de tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_tags = [\"h1\", \"h2\", \"h3\"]\n",
    "todos_headers = soup.find_all(lista_tags)\n",
    "print(todos_headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar o atributo `.name` para determinar qual o tipo de cada tag em nossa lista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(todos_headers[0].name)\n",
    "print(todos_headers[0].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Utilize uma list comprehension para criar uma lista\n",
    "# de uplas (tipo do tag, conteúdo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscando links\n",
    "\n",
    "Um **tag** específico muito útil na construção de **web crawlers** é o `<a>`. Este **tag** contém os hiper-links de uma página HTML. Vamos utilizar a BeautifulSoup para extrair todas os links de nossa notícia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:19:08.084525Z",
     "start_time": "2022-01-22T14:19:08.068546Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tag_a = soup.find_all(\"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:19:10.507391Z",
     "start_time": "2022-01-22T14:19:10.467499Z"
    }
   },
   "outputs": [],
   "source": [
    "tag_a[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:20:54.505562Z",
     "start_time": "2022-01-22T14:20:54.493597Z"
    }
   },
   "outputs": [],
   "source": [
    "first_link = tag_a[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:20:56.538026Z",
     "start_time": "2022-01-22T14:20:56.530046Z"
    }
   },
   "outputs": [],
   "source": [
    "first_link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O atributo `.text` não extrai o URL do link, apenas o texto que é exibido para o usuário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:21:15.905589Z",
     "start_time": "2022-01-22T14:21:15.893600Z"
    }
   },
   "outputs": [],
   "source": [
    "first_link.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se olharmos o *string* do **tag** poderemos entender melhor porque isso acontece. O URL em si está **dentro da declaração do tag**:\n",
    "\n",
    "    <a data-audience-click=\"\" href=\"https://www.ingresso.com/?utm_source=uol.com.br&amp;utm_medium=barrauol&amp;utm_campaign=linkfixo_barrauol&amp;utm_content=barrauol-link-ingressocom&amp;utm_term=barrauol-ingressocom\">\n",
    "\n",
    "Dentro da declaração do **tag** podemos ver algo parecido com a declaração de variáveis:\n",
    "\n",
    "    data-audience-click=\"\"\n",
    "\n",
    "e\n",
    "\n",
    "    href=\"https://www.ingresso.com/?utm_source=uol.com.br&amp;utm_medium=barrauol&amp;utm_campaign=linkfixo_barrauol&amp;utm_content=barrauol-link-ingressocom&amp;utm_term=barrauol-ingressocom\"\n",
    "\n",
    "Cada uma dessas *variáveis* é um **atributo do tag** e para extrai-las utilizaremos o atributo `.attrs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:21:57.352891Z",
     "start_time": "2022-01-22T14:21:57.332932Z"
    }
   },
   "outputs": [],
   "source": [
    "first_link.attrs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que os atributos de um **tag** são retornados como um **dict**! Se quisermos acessar uma **variável** específica podemos faze-lo através do nome desta variável:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:22:27.786564Z",
     "start_time": "2022-01-22T14:22:27.779582Z"
    }
   },
   "outputs": [],
   "source": [
    "first_link.attrs[\"href\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:29:29.870217Z",
     "start_time": "2022-01-22T14:29:29.844286Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Utilize um loop para extrair todos os URLs de\n",
    "# tags <a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tags** hierárquicos\n",
    "\n",
    "Até agora todos os **tags** que vimos eram **simples**, ou seja, não continham em seu conteúdo outros **tags**. Muitas vezes queremos extrair informações *navegando* a página **bloco a bloco**.\n",
    "\n",
    "Vamos aprender a utilizar o **inspetor de código** de um web browser para navegar blocos de tags para extrair informações complexas. Neste exemplo vamos reconstruir um tabela da Wikipedia com um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T14:59:57.149530Z",
     "start_time": "2022-01-22T14:59:57.145540Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:00:09.806119Z",
     "start_time": "2022-01-22T15:00:08.991168Z"
    }
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:00:13.027289Z",
     "start_time": "2022-01-22T15:00:12.969444Z"
    }
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:00:35.593268Z",
     "start_time": "2022-01-22T15:00:35.538416Z"
    }
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos acessar o link [List of European countries by life_expectancy](https://en.wikipedia.org/wiki/List_of_European_countries_by_life_expectancy) para entender como podemos utilizar o inspetor de código fonte para descobrir onde em nosso HTML está nossa tabela!\n",
    "\n",
    "### Extraindo a tabela completa\n",
    "\n",
    "Com o inspetor podemos ver que as tabelas desta página estão todas dentro de tags `table`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:06:36.718054Z",
     "start_time": "2022-01-22T15:06:36.703094Z"
    }
   },
   "outputs": [],
   "source": [
    "table_wiki_raw = soup.find_all(\"table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_wiki_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infelizmente, esta **tag** é muitas vezes utilizada como elemento de formatação de páginas (especialmente em página antigas).\n",
    "\n",
    "Podemos utilizar o **argumento** `attrs =` do método `.find_all()` para **filtrar** os **tags** extraídos a partir dos **valores de seus atributos**.\n",
    "\n",
    "O atributo `class` é um ótimo candidato em varias ocasiões para esse tipo de filtro. No caso da nossa tabela atual podemos ver que a classe é:\n",
    "\n",
    "    wikitable sortable static-row-numbers plainrowheaders srn-white-background jquery-tablesorter\n",
    "\n",
    "Vamos ver como utilizar o `attrs =` para realizar esse filtro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_wiki_raw = soup.find_all(\"table\", attrs={\"class\": \"wikitable\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_wiki_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:06:38.963606Z",
     "start_time": "2022-01-22T15:06:38.954630Z"
    }
   },
   "outputs": [],
   "source": [
    "print(table_wiki_raw[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navegando um **tag hierárquico**\n",
    "\n",
    "Além de poder percorrer o HTML completo utilizando o método `.find_all()`, podemos fazer buscas dentro de cada **tag**! Isso nos permite **localizar** a busca em um bloco específico delimitado por **tags**.\n",
    "\n",
    "No nosso caso atual, podemos ver que cada tabela extraída é composta por **três tags**: `<thead>`, `<tbody>` e `<tfoot>`. Como queremos extrair o **corpo** da tabela, vamos investigar o tag `tbody`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:09:21.653846Z",
     "start_time": "2022-01-22T15:09:21.635894Z"
    }
   },
   "outputs": [],
   "source": [
    "table_wiki = table_wiki_raw[3].find(\"tbody\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:09:23.735833Z",
     "start_time": "2022-01-22T15:09:23.712895Z"
    }
   },
   "outputs": [],
   "source": [
    "print(table_wiki.prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver tanto no inspetor quanto no print acima que o corpo da tabela é composto por múltiplos tags `<tr>`: se olharmos com cuidado veremos que cada `<tr>` é uma linha de nossa tabela! Vamos extrair uma linha em particular para ver do que ela é composta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:11:44.140768Z",
     "start_time": "2022-01-22T15:11:44.124810Z"
    }
   },
   "outputs": [],
   "source": [
    "table_rows = table_wiki.find_all(\"tr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:12:05.427959Z",
     "start_time": "2022-01-22T15:12:05.422970Z"
    }
   },
   "outputs": [],
   "source": [
    "print(table_rows[1].prettify())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada linha de nossa tabela é composta por uma série de tags `<td>` - e cada um destes contém as informações de uma célula. Finalmente chegamos no elemento que contém os dados da tabela!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:14:16.852925Z",
     "start_time": "2022-01-22T15:14:16.844945Z"
    }
   },
   "outputs": [],
   "source": [
    "first_row = table_rows[1].find_all(\"td\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:15:16.301740Z",
     "start_time": "2022-01-22T15:15:16.296755Z"
    }
   },
   "outputs": [],
   "source": [
    "first_row[2].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:15:55.258729Z",
     "start_time": "2022-01-22T15:15:55.237785Z"
    }
   },
   "outputs": [],
   "source": [
    "[elemento.text.strip() for elemento in first_row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos juntar todas as etapas acima em um loop (ou list comprehension) para construir nosso DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T15:54:31.881028Z",
     "start_time": "2022-01-22T15:54:31.852109Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Construir um DataFrame com os dados da tabela de Expectativa de Vida na Europa da Wikipedia\n",
    "# BONUS - faça isso com um list comprehension!\n",
    "# BONUS - contrua uma função que receba um link da Wikipedia com uma tabela e retorne um DataFrame (teste em outra tabela)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando um WebCrawler\n",
    "\n",
    "Um **webcrawler** é uma aplicação que extrai informações estruturadas a partir de multiplos sites de forma autonôma. Vamos construir um **webcrawler** para extrair um artigo (artigo origem) do jornal Guardian e qualquer outro artigo referenciado no texto do artigo origem (artigos filhos).\n",
    "\n",
    "## Extraindo o Artigo Origem\n",
    "\n",
    "O primeiro passo do nosso *crawler* é encontrar o corpo principal do artigo - caso contrário *puxaremos* links de propagandas, artigos relacionados, cabeçalho, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:33:25.076306Z",
     "start_time": "2022-01-22T18:33:24.976575Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://www.theguardian.com/world/2022/aug/18/fires-and-explosions-reported-at-military-targets-in-russia-and-crimea\"\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar buscando algum **tag** que represente o corpo do artigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:35:51.580983Z",
     "start_time": "2022-01-22T18:35:51.564009Z"
    }
   },
   "outputs": [],
   "source": [
    "artigo = soup.find(\"article\")\n",
    "print(artigo.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que encontramos um **tag** com o corpo do artigo precisamos encontrar links dentro deste corpo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:39:00.067338Z",
     "start_time": "2022-01-22T18:39:00.051353Z"
    }
   },
   "outputs": [],
   "source": [
    "links_artigo = artigo.find_all(\"a\", attrs={\"data-link-name\": \"in body link\"})\n",
    "len(links_artigo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar list comprehensions para visualizar para onde esses links nos levarão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:39:33.026587Z",
     "start_time": "2022-01-22T18:39:33.013621Z"
    }
   },
   "outputs": [],
   "source": [
    "[link[\"href\"] for link in links_artigo]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sempre que precisamos construir um loop para executar um bloco de código sobre os elementos de uma lista devemos **testar** esse bloco de código em um elemento particular da lista antes de contruir o loop completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:41:10.007214Z",
     "start_time": "2022-01-22T18:41:09.589949Z"
    }
   },
   "outputs": [],
   "source": [
    "url_filho = links_artigo[0][\"href\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que extraímos o artigo original, podemos extrair o artigo filho a partir do link extraído:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_filho = requests.get(url_filho)\n",
    "html_filho = response_filho.content\n",
    "soup_filho = BeautifulSoup(html_filho)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artigo_filho = soup_filho.find(\"article\").text\n",
    "print(artigo_filho)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos consolidar nosso código para executar o bloco acima sobre todos os *links* extraídos do artigo origem (adicionando algumas camadas de segurança para que nosso *crawler* navegue tranquilamente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:42:52.278712Z",
     "start_time": "2022-01-22T18:42:50.034162Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_artigos = []\n",
    "lista_links = [link[\"href\"] for link in links_artigo]\n",
    "for link in lista_links:\n",
    "    if link.find(\"guardian\") >= 0:\n",
    "        try:\n",
    "            response_filho = requests.get(link)\n",
    "            html_filho = response_filho.content\n",
    "            soup_filho = BeautifulSoup(html_filho)\n",
    "            lista_artigos.append(soup_filho.find(\"article\").text)\n",
    "        except AttributeError:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T18:43:06.308590Z",
     "start_time": "2022-01-22T18:43:06.293652Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_artigos[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-22T12:50:54.878340Z",
     "start_time": "2022-01-22T12:50:54.862379Z"
    }
   },
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Transforme o código acima em uma função que recebe um link de artigo do Guardian como argumento\n",
    "# e retorna uma lista com o texto do artigo original e o texto de quaisquer artigos do Guardian\n",
    "# linkados no texto do artigo original\n",
    "# BONUS - crie uma função que faça isso em profundidade maior que um: além do artigo original e dos artigos\n",
    "# filhos (artigos linkados no artigo original) trazer os artigos netos (artigos linkados nos artigos filhos)\n",
    "# BONUS BONUS - contrua uma função capaz de extrair uma profundidade arbitraria de artigos (filhos, netos, bisnetos...)\n",
    "# a partir de um parametro 'depth' (0 = original, 1 = filhos, 2 = netos, 3 = bisnetos, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conhecendo o Selenium\n",
    "\n",
    "Embora o BeautifulSoup seja uma biblioteca ótima e simples para extrair dados de páginas (o complicado são os html's...), nem sempre conseguimos usa-la: muitas páginas utilizam, hoje em dia, tecnologias incompatíveis com a arquitetura do BeautifulSoup.\n",
    "\n",
    "Páginas que são dinâmicas (especificamente, páginas que usem tecnologias client-side, como React.js) não podem ser mineradas diretamente.\n",
    "\n",
    "Para tratar destas páginas precisamos utilizar outra biblioteca: Selenium. Enquanto na BeautifulSoup carregamos o HTML da página original para dentro do Python, com a Selenium **simularemos o ato de navegação**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install selenium\n",
    "!pip3 install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a Selenium, precisamos utilizar um WebDriver - um navegador específico através do qual extraíremos as informações desejadas. Para nossa aula de hoje vamos utilizar o Chrome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=ChromeDriverManager().install())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar nosso `driver` para navegar até uma página teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://testpages.herokuapp.com/styled/index.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navegando utilizando XPATHs\n",
    "\n",
    "O forma mais fácil de navegar uma página utilizando Selenium é através de XPATHs: identificadores únicos dos elementos de uma página. Vamos ver como podemos descobrir um XPATH de um elemento particular e como utilizar o nosso `driver` para interagir com o elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_htmlformtest = driver.find_element(\n",
    "    By.XPATH, \"/html/body/div/ul[1]/li[7]/ul/li[1]/a\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `.find_element()` nos permite encontrar elementos em uma página (parecido com o `.find_all()` do BS) - mas por si só não nos diz muito sobre o elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_htmlformtest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos buscar os atributos do elemento utilizando o método `.get_attribute()` - vamos utilizar este método para extrair o URL do link associado ao elemento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_htmlformtest.get_attribute(\"href\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora tudo está muito parecido com a BS - agora vamos interagir com a página, *clicando* virtualmente no link:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_htmlformtest.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interagindo com Formulários\n",
    "\n",
    "Um dos principais usos para o Selenium é conseguir preencher de forma programática formulários em páginas. Vamos continuar utilizando nosso driver para preencher o formulário na página para qual navegamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_username = driver.find_element(\n",
    "    By.XPATH, \"/html/body/div/div[3]/form/table/tbody/tr[1]/td/input\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar o método `.send_keys()` para *preencher* um formulário: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_username.send_keys(\"aaaa\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e o método `.clear()` para *limpar* o formulário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_username.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como a variável `input_username` ainda é o mesmo elemento (o campo **Username**), podemos voltar a preencher o formulário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_usuario = \"pedrotechel\"\n",
    "input_username.send_keys(nome_usuario)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementos clicáveis, como botões ou checkboxes podem ser acessados através do método click:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_opt1 = driver.find_element(\n",
    "    By.XPATH, \"/html/body/div/div[3]/form/table/tbody/tr[5]/td/input[1]\"\n",
    ")\n",
    "input_opt1.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_rdbx1 = driver.find_element(\n",
    "    By.XPATH, \"/html/body/div/div[3]/form/table/tbody/tr[6]/td/input[1]\"\n",
    ")\n",
    "input_rdbx1.click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo Prático - Mercado Livre\n",
    "\n",
    "Vamos construir um *webcrawler*, baseado em Selenium, para buscar preços de azeite no Mercado Livre. Plataformas de e-commerce quase sempre utilizam tecnologias que impossibilitam a utilização do BeautifulSoup!\n",
    "\n",
    "Além de *apontar* o driver para a página principal do Mercado Livre, vamos utilizar o método `.implicitly_wait()` para que o driver *espere* um tempo até todos os elementos da página renderizarem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.mercadolivre.com.br/\")\n",
    "driver.implicitly_wait(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos interagir com a barra de busca para procurar azeites. Muitas barras de busca tem testos pré-preenchidos, então antes de continuarmos com nossa busca vamos limpar a barra utilizando o método `.clear()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barra_busca = driver.find_element(By.XPATH, \"/html/body/header/div/div[2]/form/input\")\n",
    "barra_busca.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a barra limpa, podemos utilizar o método `.send_keys()` para preenche-la com o nome do produto que queremos buscar (`\"azeite\"`) e confirmar a busca utilizando o objeto `Keys.ENTER` (simulando digitar \"azeite\" e apertar *enter* na barra de busca): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barra_busca.send_keys(\"azeite\")\n",
    "barra_busca.send_keys(Keys.ENTER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Até agora tranquilo! Para continuarmos, no entanto, precisaremos extrair TODOS os preços de produtos. Para isso utilizaremos o método `.find_elements()` (no plural) e, ao invés de utilizar o XPATH, buscaremos pelo **tag** e sua **classe**. Vamos começar encontrando todas as *caixas de produto*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_elem_produto = driver.find_elements(\n",
    "    By.CLASS_NAME, \"ui-search-result__content-wrapper\"\n",
    ")\n",
    "len(lista_elem_produto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lista construída acima contém `WebElements` - da mesma forma que podemos fazer buscas dentro de **tags** hierarquicas utilizando o BS, podemos fazer buscas dentro de diferentes `WebElements` utilizando o Selenium:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_elem_preco = []\n",
    "for produto in lista_elem_produto:\n",
    "    caixa_preco = produto.find_element(By.CLASS_NAME, \"price-tag-amount\")\n",
    "    lista_elem_preco.append(caixa_preco)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o atributo `.text` para extrair o conteúdo de cada uma das caixas de preço (primeiro testando com um elemento da lista):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preco_teste = lista_elem_preco[0]\n",
    "preco_teste.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O preço extraído acima parece não bater com o preço da página! No entanto, se prestarmos atenção, veremos que este é o preço sem desconto do produto (preço cheio).\n",
    "\n",
    "Vamos alterar nosso loop para extrair todos os preços de cada produto (cheio e descontado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_elem_preco = []\n",
    "for produto in lista_elem_produto:\n",
    "    caixa_preco = produto.find_elements(By.CLASS_NAME, \"price-tag-amount\")\n",
    "    lista_elem_preco.append(caixa_preco)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_elem_preco[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos 3 preços associados à este produto... Vamos analisar o que cada um deles representa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[preco.text for preco in lista_elem_preco[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro preço representa o preço cheio, o segundo com desconto e o terceiro é o preço de cada parcela. Este último não representa muita coisa, já que o número de parcelas pode variar entre produtos. Vamos tratar os strings dos dois primeiros, transformado-os em floats, em uma lista de preços:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_preco(str_preco):\n",
    "    pattern = r\"[^0-9.,]\"\n",
    "    return float(re.sub(pattern, \"\", str_preco).replace(\",\", \".\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[limpar_preco(preco.text) for preco in lista_elem_preco[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos construir nosso loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_preco_cheio = []\n",
    "lista_preco_desconto = []\n",
    "pattern = r\"[^0-9.,]\"\n",
    "\n",
    "for elem_preco in lista_elem_preco:\n",
    "    precos = [limpar_preco(preco.text) for preco in elem_preco]\n",
    "    lista_preco_cheio.append(precos[0])\n",
    "    lista_preco_desconto.append(precos[1])\n",
    "\n",
    "print(lista_preco_cheio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos investigar a lista de preços com desconto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lista_preco_desconto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alguns preços estão muito abaixo do esperado! Se compararmos os resultados com a página veremos que nem todos os produtos tem descontos. Vamos investigar um preço que tenha essa caracteristica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[limpar_preco(preco.text) for preco in lista_elem_preco[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando um produto não está promocionado, o segundo elemento da lista preço é o valor da parcela para compras parceladas! Vamos alterar nosso loop para contemplar essa condição, guardando `np.nan` no preço descontado de produtos sem desconto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_preco_cheio = []\n",
    "lista_preco_desconto = []\n",
    "pattern = r\"[^0-9.,]\"\n",
    "\n",
    "for elem_preco in lista_elem_preco:\n",
    "    if len(elem_preco) == 3:\n",
    "        precos = [limpar_preco(preco.text) for preco in elem_preco]\n",
    "        lista_preco_cheio.append(precos[0])\n",
    "        lista_preco_desconto.append(precos[1])\n",
    "    else:\n",
    "        print(\"Produto sem promoção!\")\n",
    "        lista_preco_cheio.append(precos[0])\n",
    "        lista_preco_desconto.append(np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar o mesmo método para construir a lista de nomes de produto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_elem_produto[0].find_element(By.CLASS_NAME, \"ui-search-item__title\").text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_nome = []\n",
    "for elem_nome in lista_elem_produto:\n",
    "    nome = elem_nome.find_element(By.CLASS_NAME, \"ui-search-item__title\")\n",
    "    lista_nome.append(nome.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos juntar nossas duas listas em um DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_azeite = pd.DataFrame(\n",
    "    {\n",
    "        \"nome\": lista_nome,\n",
    "        \"preco_cheio\": lista_preco_cheio,\n",
    "        \"preco_desconto\": lista_preco_desconto,\n",
    "    }\n",
    ")\n",
    "tb_azeite.head()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
